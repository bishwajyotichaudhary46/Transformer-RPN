{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Deeplearning\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#components\n",
    "from Transformer_RPN.components.vision_transformer import VisionTransformer\n",
    "from Transformer_RPN.components.region_proposal_network import RegionProposalNetwork\n",
    "from Transformer_RPN.components.roi_head import ROIHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(path):\n",
    "    with open(path) as yaml_file:\n",
    "        content = yaml.safe_load(yaml_file)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = read_yaml('config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingestion = content['data_ingestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_dir': 'Data/Strawberry data/strawberry_data',\n",
       " 'xml_dir': 'Data/Strawberry data/strawberry_labels',\n",
       " 'class': ['angular_leafspot',\n",
       "  'anthracnose_fruit_rot',\n",
       "  'gray_mold',\n",
       "  'leaf_scorch',\n",
       "  'leaf_spot']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    def __init__(self, img_dir, xml_dir, label2idx):\n",
    "        self.img_dir = img_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.label2idx = label2idx\n",
    "        self.img_infos = []\n",
    "\n",
    "    def load(self):\n",
    "        xml_files = [os.path.join(self.xml_dir, dir, file) for dir in os.listdir(self.xml_dir) for file in os.listdir(os.path.join(self.xml_dir, dir))]\n",
    "        for file in tqdm(xml_files, desc='Processing XML files'):\n",
    "            img_info = {}\n",
    "            img_info['id'] = os.path.basename(file).split('.xml')[0]\n",
    "            xml_info = ET.parse(file)\n",
    "            root = xml_info.getroot()\n",
    "            size = root.find('size')\n",
    "            folder = file.split('/')[3]\n",
    "            img_info['image'] = os.path.join(self.img_dir, folder,'{}.jpg'.format(img_info['id']))\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            img_info['height'] = height\n",
    "            img_info['width'] = width\n",
    "            detections = []\n",
    "        \n",
    "\n",
    "            for obj in xml_info.findall('object'):\n",
    "                det = {}\n",
    "                label = label2idx[obj.find('name').text]\n",
    "                if obj.find('name').text == 'leaf_blight':\n",
    "                    print(img_info['id'])\n",
    "                bbox_info = obj.find('bndbox')\n",
    "                bbox = [\n",
    "                    int(float(bbox_info.find('xmin').text))-1,\n",
    "                    int(float(bbox_info.find('ymin').text))-1,\n",
    "                    int(float(bbox_info.find('xmax').text))-1,\n",
    "                    int(float(bbox_info.find('ymax').text))-1\n",
    "                ]\n",
    "                det['label'] = label\n",
    "                det['bbox'] = bbox\n",
    "                detections.append(det)\n",
    "            \n",
    "            img_info['detections'] = detections\n",
    "            self.img_infos.append(img_info)\n",
    "        return self.img_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data_ingestion['class']\n",
    "classes = sorted(classes)\n",
    "classes = ['background'] + classes\n",
    "label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "idx2label = {idx: classes[idx] for idx in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files:   0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 941/941 [00:00<00:00, 4088.10it/s]\n"
     ]
    }
   ],
   "source": [
    "load_data = LoadData(img_dir=data_ingestion['img_dir'], xml_dir=data_ingestion['xml_dir'], label2idx=label2idx)\n",
    "data = load_data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len_data = len(data)\n",
    "total_len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 658\n",
      "Validation Size : 141\n",
      "Test_size: 142\n"
     ]
    }
   ],
   "source": [
    "train_size = int(total_len_data * 0.7)\n",
    "val_size = int(total_len_data * 0.15)\n",
    "test_size = total_len_data - train_size - val_size\n",
    "print(f\"Train Size: {train_size}\\nValidation Size : {val_size}\\nTest_size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size: train_size+val_size]\n",
    "test_data = data[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.target_size = (224,224)\n",
    "        self.images_info = data\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(self.target_size),  # Resize images\n",
    "            torchvision.transforms.ToTensor()  # Convert to tensor\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_info = self.images_info[index]\n",
    "        img = Image.open(img_info['image'])\n",
    "        \n",
    "        # Store original dimensions\n",
    "        original_w, original_h = img.size\n",
    "        target_w, target_h = self.target_size\n",
    "    \n",
    "        # Resize the image\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        # Scale bounding boxes to new image size\n",
    "        targets = {}\n",
    "        targets['bboxes'] = []\n",
    "        targets['labels'] = torch.as_tensor([d['label'] for d in img_info['detections']], dtype=torch.int64)\n",
    "\n",
    "        scale_x = target_w / original_w\n",
    "        scale_y = target_h / original_h\n",
    "\n",
    "        for detection in img_info['detections']:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "\n",
    "            # Scale bbox to new dimensions\n",
    "            x1 = int(x1 * scale_x)\n",
    "            y1 = int(y1 * scale_y)\n",
    "            x2 = int(x2 * scale_x)\n",
    "            y2 = int(y2 * scale_y)\n",
    "\n",
    "            targets['bboxes'].append([x1, y1, x2, y2])\n",
    "\n",
    "        targets['bboxes'] = torch.as_tensor(targets['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, targets, img_info['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = CustomDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = CustomDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_datasets,batch_size=1,shuffle=True,num_workers=4)\n",
    "test_dl = DataLoader(test_datasets,batch_size=1,shuffle=True,num_workers=4)\n",
    "val_dl = DataLoader(val_datasets,batch_size=1,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  8., 132.,  18., 142.],\n",
      "         [  0., 114.,   9., 122.],\n",
      "         [ 58., 123.,  66., 132.],\n",
      "         [ 90., 171.,  98., 180.],\n",
      "         [ 87., 148.,  96., 155.],\n",
      "         [127., 170., 134., 181.],\n",
      "         [133., 154., 140., 164.],\n",
      "         [116., 144., 124., 151.],\n",
      "         [124., 104., 130., 114.],\n",
      "         [142., 136., 162., 162.],\n",
      "         [148., 126., 155., 135.],\n",
      "         [174., 164., 179., 175.],\n",
      "         [151., 175., 158., 183.],\n",
      "         [148., 183., 155., 191.],\n",
      "         [152., 183., 162., 203.],\n",
      "         [103., 178., 115., 192.],\n",
      "         [109., 168., 116., 178.],\n",
      "         [ 84.,  73., 109., 128.],\n",
      "         [116.,  33., 122.,  41.],\n",
      "         [109., 105., 114., 112.],\n",
      "         [ 22., 106.,  28., 112.],\n",
      "         [ 71., 158.,  78., 165.],\n",
      "         [188., 180., 195., 189.],\n",
      "         [140., 162., 149., 173.],\n",
      "         [109., 160., 115., 167.],\n",
      "         [ 99., 152., 109., 164.],\n",
      "         [103., 135., 111., 141.],\n",
      "         [ 61., 140.,  74., 148.],\n",
      "         [ 17., 125.,  24., 135.],\n",
      "         [ 22., 151.,  33., 158.],\n",
      "         [118., 159., 124., 164.],\n",
      "         [121., 167., 127., 174.],\n",
      "         [165., 133., 171., 140.],\n",
      "         [177., 143., 183., 149.],\n",
      "         [ 72., 101.,  80., 112.]]])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for image, target, _ in tqdm(train_dl):\n",
    "    print(target['bboxes'])\n",
    "    print(image.shape)\n",
    "    image = image\n",
    "    target = target\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': tensor([[[  8., 132.,  18., 142.],\n",
       "          [  0., 114.,   9., 122.],\n",
       "          [ 58., 123.,  66., 132.],\n",
       "          [ 90., 171.,  98., 180.],\n",
       "          [ 87., 148.,  96., 155.],\n",
       "          [127., 170., 134., 181.],\n",
       "          [133., 154., 140., 164.],\n",
       "          [116., 144., 124., 151.],\n",
       "          [124., 104., 130., 114.],\n",
       "          [142., 136., 162., 162.],\n",
       "          [148., 126., 155., 135.],\n",
       "          [174., 164., 179., 175.],\n",
       "          [151., 175., 158., 183.],\n",
       "          [148., 183., 155., 191.],\n",
       "          [152., 183., 162., 203.],\n",
       "          [103., 178., 115., 192.],\n",
       "          [109., 168., 116., 178.],\n",
       "          [ 84.,  73., 109., 128.],\n",
       "          [116.,  33., 122.,  41.],\n",
       "          [109., 105., 114., 112.],\n",
       "          [ 22., 106.,  28., 112.],\n",
       "          [ 71., 158.,  78., 165.],\n",
       "          [188., 180., 195., 189.],\n",
       "          [140., 162., 149., 173.],\n",
       "          [109., 160., 115., 167.],\n",
       "          [ 99., 152., 109., 164.],\n",
       "          [103., 135., 111., 141.],\n",
       "          [ 61., 140.,  74., 148.],\n",
       "          [ 17., 125.,  24., 135.],\n",
       "          [ 22., 151.,  33., 158.],\n",
       "          [118., 159., 124., 164.],\n",
       "          [121., 167., 127., 174.],\n",
       "          [165., 133., 171., 140.],\n",
       "          [177., 143., 183., 149.],\n",
       "          [ 72., 101.,  80., 112.]]]),\n",
       " 'labels': tensor([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4,\n",
       "          4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 4]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = VisionTransformer().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature = vision_model(image.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_params = read_yaml(\"params.yaml\")['rpn_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scales': [128, 256, 512],\n",
       " 'ascpect_ratios': [0.5, 1, 2],\n",
       " 'low_iou_threshold': 0.3,\n",
       " 'high_iou_threshold': 0.7,\n",
       " 'rpn_nms_threshold': 0.7,\n",
       " 'rpn_batch_size': 256,\n",
       " 'rpn_prenms_train_topk': 12000,\n",
       " 'rpn_prenms_test_topk': 3000,\n",
       " 'rpn_train_topk': 2000,\n",
       " 'rpn_test_topk': 300,\n",
       " 'input_channels': 768}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_model = RegionProposalNetwork(ascpect_ratios=rpn_params['ascpect_ratios'], scales=rpn_params['scales'], in_channels=rpn_params['input_channels'], rpn_prenms_topk=rpn_params['rpn_prenms_train_topk'], rpn_nms_threshold=rpn_params['rpn_nms_threshold'], rpn_topk=rpn_params['rpn_train_topk'], high_iou_threshold=rpn_params['high_iou_threshold'],low_iou_threshold=rpn_params['low_iou_threshold']).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_output = rpn_model(image, base_feature, target=target, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposals': tensor([[  0.,   0., 224., 224.]], device='cuda:0'),\n",
       " 'scores': tensor([1.], device='cuda:0'),\n",
       " 'rpn_classification_loss': tensor(240.4287, device='cuda:0',\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'rpn_localization_loss': tensor(1369.7397, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head_param = read_yaml('params.yaml')['roi_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_classes': 6,\n",
       " 'roi_batch_size': 128,\n",
       " 'fc_inner_dim': 1024,\n",
       " 'roi_iou_threshold': 0.5,\n",
       " 'roi_low_bg_iou': 0.0,\n",
       " 'roi_pool_size': 7,\n",
       " 'roi_nms_threshold': 0.3,\n",
       " 'roi_topk_detections': 100,\n",
       " 'roi_score_threshold': 0.05,\n",
       " 'roi_pos_fraction': 0.25}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_head_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head_model = ROIHead(model_config=roi_head_param, num_classes=roi_head_param['num_classes'],in_channels=768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0625, 0.0625]\n"
     ]
    }
   ],
   "source": [
    "frcnn_output = roi_head_model(base_feature, rpn_output['proposals'], image.shape[-2:], target, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frcnn_classification_loss': tensor(343.4810, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'frcnn_localization_loss': tensor(143.5847, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frcnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer_RPN.pipeline.model import TransformerRPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerRPN(roi_head_param=roi_head_param, rpn_params=rpn_params, device=device, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerRPN(\n",
       "  (vision_model): VisionTransformer(\n",
       "    (transformer_encoder_layers): Sequential(\n",
       "      (0): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): TransformerEncoder(\n",
       "        (MSA): MultiHeadSelfAttention(\n",
       "          (multihead_attention): ModuleList(\n",
       "            (0-11): 12 x SelfAttention()\n",
       "          )\n",
       "        )\n",
       "        (MLP): MultiLayerPerceptron(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rpn_model): RegionProposalNetwork(\n",
       "    (rpn_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cls_layer): Conv2d(768, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bbox_reg_layer): Conv2d(768, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (roi_head_model): ROIHead(\n",
       "    (fc6): Linear(in_features=37632, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (cls_layer): Linear(in_features=1024, out_features=6, bias=True)\n",
       "    (bbox_reg_layer): Linear(in_features=1024, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0625, 0.0625]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'proposals': tensor([[  0.0000,   0.0000, 224.0000, 224.0000],\n",
       "          [  0.0000,   0.0000, 114.9902, 224.0000]], device='cuda:0'),\n",
       "  'scores': tensor([1., 0.], device='cuda:0'),\n",
       "  'rpn_classification_loss': tensor(267.4563, device='cuda:0',\n",
       "         grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'rpn_localization_loss': tensor(1607.2919, device='cuda:0', grad_fn=<DivBackward0>)},\n",
       " {'frcnn_classification_loss': tensor(324.4942, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       "  'frcnn_localization_loss': tensor(125.7053, device='cuda:0', grad_fn=<DivBackward0>)})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image = image, target=target, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
