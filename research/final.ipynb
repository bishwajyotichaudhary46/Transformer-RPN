{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Deeplearning\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#components\n",
    "from Transformer_RPN.components.vision_transformer import VisionTransformer\n",
    "from Transformer_RPN.components.anchor_genrate import GenerateAnchor\n",
    "from Transformer_RPN.components.region_proposal_network import RegionProposalNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(path):\n",
    "    with open(path) as yaml_file:\n",
    "        content = yaml.safe_load(yaml_file)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = read_yaml('config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingestion = content['data_ingestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_dir': 'Data/Strawberry data/strawberry_data',\n",
       " 'xml_dir': 'Data/Strawberry data/strawberry_labels',\n",
       " 'class': ['angular_leafspot',\n",
       "  'anthracnose_fruit_rot',\n",
       "  'gray_mold',\n",
       "  'leaf_scorch',\n",
       "  'leaf_spot']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    def __init__(self, img_dir, xml_dir, label2idx):\n",
    "        self.img_dir = img_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.label2idx = label2idx\n",
    "        self.img_infos = []\n",
    "\n",
    "    def load(self):\n",
    "        xml_files = [os.path.join(self.xml_dir, dir, file) for dir in os.listdir(self.xml_dir) for file in os.listdir(os.path.join(self.xml_dir, dir))]\n",
    "        for file in tqdm(xml_files, desc='Processing XML files'):\n",
    "            img_info = {}\n",
    "            img_info['id'] = os.path.basename(file).split('.xml')[0]\n",
    "            xml_info = ET.parse(file)\n",
    "            root = xml_info.getroot()\n",
    "            size = root.find('size')\n",
    "            folder = file.split('/')[3]\n",
    "            img_info['image'] = os.path.join(self.img_dir, folder,'{}.jpg'.format(img_info['id']))\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            img_info['height'] = height\n",
    "            img_info['width'] = width\n",
    "            detections = []\n",
    "        \n",
    "\n",
    "            for obj in xml_info.findall('object'):\n",
    "                det = {}\n",
    "                label = label2idx[obj.find('name').text]\n",
    "                if obj.find('name').text == 'leaf_blight':\n",
    "                    print(img_info['id'])\n",
    "                bbox_info = obj.find('bndbox')\n",
    "                bbox = [\n",
    "                    int(float(bbox_info.find('xmin').text))-1,\n",
    "                    int(float(bbox_info.find('ymin').text))-1,\n",
    "                    int(float(bbox_info.find('xmax').text))-1,\n",
    "                    int(float(bbox_info.find('ymax').text))-1\n",
    "                ]\n",
    "                det['label'] = label\n",
    "                det['bbox'] = bbox\n",
    "                detections.append(det)\n",
    "            \n",
    "            img_info['detections'] = detections\n",
    "            self.img_infos.append(img_info)\n",
    "        return self.img_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data_ingestion['class']\n",
    "classes = sorted(classes)\n",
    "classes = ['background'] + classes\n",
    "label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "idx2label = {idx: classes[idx] for idx in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files:   0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 941/941 [00:00<00:00, 7323.56it/s]\n"
     ]
    }
   ],
   "source": [
    "load_data = LoadData(img_dir=data_ingestion['img_dir'], xml_dir=data_ingestion['xml_dir'], label2idx=label2idx)\n",
    "data = load_data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len_data = len(data)\n",
    "total_len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 658\n",
      "Validation Size : 141\n",
      "Test_size: 142\n"
     ]
    }
   ],
   "source": [
    "train_size = int(total_len_data * 0.7)\n",
    "val_size = int(total_len_data * 0.15)\n",
    "test_size = total_len_data - train_size - val_size\n",
    "print(f\"Train Size: {train_size}\\nValidation Size : {val_size}\\nTest_size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size: train_size+val_size]\n",
    "test_data = data[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.target_size = (224,224)\n",
    "        self.images_info = data\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(self.target_size),  # Resize images\n",
    "            torchvision.transforms.ToTensor()  # Convert to tensor\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_info = self.images_info[index]\n",
    "        img = Image.open(img_info['image'])\n",
    "        \n",
    "        # Store original dimensions\n",
    "        original_w, original_h = img.size\n",
    "        target_w, target_h = self.target_size\n",
    "    \n",
    "        # Resize the image\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        # Scale bounding boxes to new image size\n",
    "        targets = {}\n",
    "        targets['bboxes'] = []\n",
    "        targets['labels'] = torch.as_tensor([d['label'] for d in img_info['detections']], dtype=torch.int64)\n",
    "\n",
    "        scale_x = target_w / original_w\n",
    "        scale_y = target_h / original_h\n",
    "\n",
    "        for detection in img_info['detections']:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "\n",
    "            # Scale bbox to new dimensions\n",
    "            x1 = int(x1 * scale_x)\n",
    "            y1 = int(y1 * scale_y)\n",
    "            x2 = int(x2 * scale_x)\n",
    "            y2 = int(y2 * scale_y)\n",
    "\n",
    "            targets['bboxes'].append([x1, y1, x2, y2])\n",
    "\n",
    "        targets['bboxes'] = torch.as_tensor(targets['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, targets, img_info['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = CustomDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = CustomDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_datasets,batch_size=1,shuffle=True,num_workers=4)\n",
    "test_dl = DataLoader(test_datasets,batch_size=1,shuffle=True,num_workers=4)\n",
    "val_dl = DataLoader(val_datasets,batch_size=1,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 72.,  26., 158., 179.]]])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/658 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for image, target, _ in tqdm(train_dl):\n",
    "    print(target['bboxes'])\n",
    "    print(image.shape)\n",
    "    image = image\n",
    "    target = target\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': tensor([[[ 72.,  26., 158., 179.]]]), 'labels': tensor([[3]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = VisionTransformer().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature = vision_model(image.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_params = read_yaml(\"params.yaml\")['rpn_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scales': [128, 256, 512],\n",
       " 'ascpect_ratios': [0.5, 1, 2],\n",
       " 'low_iou_threshold': 0.3,\n",
       " 'high_iou_threshold': 0.7,\n",
       " 'rpn_nms_threshold': 0.7,\n",
       " 'rpn_batch_size': 256,\n",
       " 'rpn_prenms_train_topk': 12000,\n",
       " 'rpn_prenms_test_topk': 3000,\n",
       " 'rpn_train_topk': 2000,\n",
       " 'rpn_test_topk': 300,\n",
       " 'input_channels': 768}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_model = RegionProposalNetwork(rpn_params['ascpect_ratios'], rpn_params['scales'], rpn_params['input_channels']).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_score, regression_bbox, anchors = rpn_model(image, base_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1764, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -27397.1426,  -81952.3516,  -16022.1426,  -76327.3516]],\n",
       "\n",
       "        [[-422985.8125,    1853.2192, -422985.8125,   13228.2207]],\n",
       "\n",
       "        [[ 418186.1875, -186198.1406,  463436.1875, -186198.1406]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-128415.0000,   46086.1484, -128415.0000,   46086.1484]],\n",
       "\n",
       "        [[-131749.6250,  157908.2656, -131749.6250,  180533.2656]],\n",
       "\n",
       "        [[ -95510.2031, -907808.8125,  -95510.2031, -907808.8125]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
